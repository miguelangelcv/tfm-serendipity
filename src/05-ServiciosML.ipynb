{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "eb690c1d-59af-4ecf-8427-17f270e7d13b",
   "metadata": {},
   "source": [
    "<img src=\"images/header-transparent.png\" alt=\"Logo UCLM-ESII\" align=\"right\">\n",
    "\n",
    "<br><br><br><br>\n",
    "<h2><font color=\"#92002A\" size=4>Trabajo Fin de Máster</font></h2>\n",
    "\n",
    "<h1><font color=\"#6B001F\" size=5>SERENDIPITY: Servicio web para la recomendacIón de playlists a partir de otra playlist</font></h1>\n",
    "<h2><font color=\"#92002A\" size=3>Parte 5 - Servicios de Machine Learning</font></h2>\n",
    "\n",
    "<br>\n",
    "<div style=\"text-align: right\">\n",
    "    <font color=\"#B20033\" size=3><strong>Autor</strong>: <em>Miguel Ángel Cantero Víllora</em></font><br>\n",
    "    <br>\n",
    "    <font color=\"#B20033\" size=3><strong>Directores</strong>: <em>José Antonio Gámez Martín</em></font><br>\n",
    "    <font color=\"#B20033\" size=3><em>Juan Ángel Aledo Sánchez</em></font><br>\n",
    "    <br>\n",
    "<font color=\"#B20033\" size=3>Máster Universitario en Ingeniería Informática</font><br>\n",
    "<font color=\"#B20033\" size=2>Escuela Superior de Ingeniería Informática | Universidad de Castilla-La Mancha</font>\n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "403ecce1-5553-43ee-bd8a-191ae4521b1e",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "<br>\n",
    "\n",
    "\n",
    "<a id=\"indice\"></a>\n",
    "<h2><font color=\"#92002A\" size=5>Índice</font></h2>\n",
    "\n",
    "<br>\n",
    "\n",
    "* [1. Introducción](#section1)\n",
    "* [2. Acceso al área de trabajo de aprendizaje automático](#section2)\n",
    "* [3. Obtención del modelo](#section2)\n",
    "* [4. Implementación del script de entrada](#section3)\n",
    "* [5. Creación del punto de conexión](#section4)\n",
    "\n",
    "<br>\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b1d195ad-e297-4d1a-a8fb-e0f20cfc0155",
   "metadata": {},
   "source": [
    "<br>\n",
    "\n",
    "\n",
    "<a id=\"section1\"></a>\n",
    "## <font color=\"#92002A\">1 - Introducción</font>\n",
    "<br>\n",
    "\n",
    "En esta libreta, vamos a hacer uso de los servicios de machine learning (*MLaaS*) que ofrece *Microsoft Azure* para crear un punto de conexión, desde el cual podremos utilizar el modelo entrenado para la predicción de playlist desde la *API REST* que desarrollemos más adelante.\n",
    "\n",
    "Como vimos en la libreta anterior, creamos un área de trabajo de machine learning y una instancia de cómputo para entrenar nuestro modelo. únicamente nos faltaría registrar el modelo dentro del área de trabajo y montar el *endpoint* que hará uso de él. Esta tarea la realizaremos de forma programática desde la libreta.\n",
    "\n",
    "\n",
    "<br>\n",
    "\n",
    "<div class=\"alert alert-block alert-warning\">\n",
    "    \n",
    "<i class=\"fa fa-exclamation-circle\" aria-hidden=\"true\"></i>\n",
    "    <strong>Importante</strong>: Aunque emplearemos las librerías de <i>Python</i> que ofrece <i>Microsoft</i> para hacer uso de los servicios de machine learning, es necesario que instalemos la <a href=\"https://docs.microsoft.com/es-es/cli/azure/install-azure-cli\"><strong>Interfaz de la línea de comandos de Azure</strong></a> (también conocida como <i>Azure CLI</i>). Una vez instalada, debemos iniciar sesión en nuestra cuenta de Azure desde la línea de comandos. Una vez realizado este proceso, ya podremos utilizar sin problemas las librerías de <i>Azure ML</i> en <i>Python</i>.\n",
    "</div>\n",
    "\n",
    "<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f40984e-b80e-4f5f-a728-a37eee7ee219",
   "metadata": {},
   "source": [
    "<div style=\"text-align: right\">\n",
    "<a href=\"#indice\"><font size=5><i class=\"fa fa-arrow-circle-up\" aria-hidden=\"true\" style=\"color:#92002A\"></i></font></a>\n",
    "</div>\n",
    "\n",
    "---\n",
    "\n",
    "<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b6c19811-15f6-4551-a4c9-5776c67612c0",
   "metadata": {},
   "source": [
    "<a id=\"section2\"></a>\n",
    "## <font color=\"#92002A\">2 - Acceso al área de trabajo de aprendizaje automático</font>\n",
    "\n",
    "<br>\n",
    "\n",
    "En esta sección, vamos a autenticarnos con nuestra cuenta de *Azure* para obtener acceso al área de trabajo de machine learning que tenemos creada. Podemos encontrar más información en la siguiente libreta de ejemplo: [Authentication in AzureML](https://github.com/Azure/MachineLearningNotebooks/blob/master/how-to-use-azureml/manage-azureml-service/authentication-in-azureml/authentication-in-azureml.ipynb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "4157ac2d-3aa0-4551-b0d4-4c218b17533a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install azureml azureml.core azureml-core"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ecc2f54e-00b0-49f8-b3ec-57eced4c54cf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Encontrado el área de trabajo 'serendipity-ml-workspace' en la región 'northeurope'.\n"
     ]
    }
   ],
   "source": [
    "from azureml.core import Workspace\n",
    "from azureml.core.authentication import AzureCliAuthentication\n",
    "\n",
    "cli_auth = AzureCliAuthentication()\n",
    "\n",
    "subscription_id = \"<<INSERTAR IDENTIFICADOR>>\" # Identificador de nuestra suscripción\n",
    "resource_group = \"<<INSERTAR NOMBRE DEL GRUPO DE RECURSOS>>\" # Nombre del grupo de recursos donde esta el área de trabajo\n",
    "workspace_name = \"<<INSERTAR NOMBRE DEL ÁREA DE TRABAJO>>\" # Nombre del área de trabajo\n",
    "\n",
    "ws = Workspace(subscription_id, resource_group, workspace_name, auth=cli_auth)\n",
    "ws.write_config()\n",
    "\n",
    "print(\"Encontrado el área de trabajo '{}' en la región '{}'.\".format(ws.name, ws.location))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c6af3c9-a9f7-4f48-bda4-6b1af7e85a21",
   "metadata": {},
   "source": [
    "<br>\n",
    "\n",
    "A continuación. procedemos a registrar el modelo en la colección de modelos del área de trabajo de machine learning. Para identificarlo, le estableceremos un nombre, el framework del modelo y su correspondiente versión:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef6dc7e8-3f75-43c1-a712-3b5bfc7946b4",
   "metadata": {},
   "source": [
    "<div style=\"text-align: right\">\n",
    "<a href=\"#indice\"><font size=5><i class=\"fa fa-arrow-circle-up\" aria-hidden=\"true\" style=\"color:#92002A\"></i></font></a>\n",
    "</div>\n",
    "\n",
    "---\n",
    "\n",
    "<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da50698d-4204-4d70-a151-dcaf334834b1",
   "metadata": {},
   "source": [
    "<a id=\"section3\"></a>\n",
    "## <font color=\"#92002A\">3 - Obtención del modelo</font>\n",
    "\n",
    "<br>\n",
    "\n",
    "Cuando realizamos el entrenamiento del modelo *LightFM* en la instancia de cómputo, empleando el script definido en la libreta *[Parte 3 - Modelo de recomendación](./03-ModeloRecomendacion.ipynb)*, incorporamos un fragmento de código para registrar dicho modelo en la biblioteca de modelos de nuestra área de trabajo. A continuación, vamos a importar el modelo que hemos empleado para crear el _endpoint_. En caso de que no se encontrara dicho modelo, por cualquier motivo, accederemos a la carpeta `model` de nuestra máquina y lo importaremos nuevamente:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "bf970078-3eb6-4b32-8a8a-00bbba006a11",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Encontrado modelo 'serendipity-recsys-model', versión 1\n"
     ]
    }
   ],
   "source": [
    "from azureml.core.model import Model\n",
    "\n",
    "model_name = 'serendipity-recsys-model'\n",
    "model_version = 1\n",
    "\n",
    "if model_name not in [m.name for m in  Model.list(workspace=ws)]:\n",
    "    print(\"Modelo no encontrado. Registrando ...\")\n",
    "    model = Model.register(\n",
    "        workspace = ws,\n",
    "        model_path ='./model',\n",
    "        model_name = model_name,\n",
    "        model_framework=\"LightFM\",\n",
    "        model_framework_version=\"1.16.0\",\n",
    "        tags = {\"version\": model_version},\n",
    "        description = \"Serenditipy LightFM model for playlists recommendation (tracks)\"\n",
    "    )\n",
    "    print(\"Modelo registrado\")\n",
    "else:\n",
    "    for model in Model.list(workspace=ws):\n",
    "        if model.name == model_name:\n",
    "            model_version = str(model.version)\n",
    "    print(f\"Encontrado modelo '{model_name}', versión {model_version}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32338c1a-b6e1-4813-8a41-7ad946f60078",
   "metadata": {},
   "source": [
    "<br>\n",
    "\n",
    "<div class=\"alert alert-info\">\n",
    "\n",
    "<i class=\"fa fa-info-circle\" aria-hidden=\"true\"></i>\n",
    "__Nota__: Debido al tamaño de nuestro modelo, en caso de necesitar registrarlo nuevamente, el proceso puede demorarse dependiendo de nuestra conexión a Internet.\n",
    "</div>\n",
    "\n",
    "<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "52798589-e97f-4023-b163-ef879ab717ad",
   "metadata": {},
   "source": [
    "<div style=\"text-align: right\">\n",
    "<a href=\"#indice\"><font size=5><i class=\"fa fa-arrow-circle-up\" aria-hidden=\"true\" style=\"color:#92002A\"></i></font></a>\n",
    "</div>\n",
    "\n",
    "---\n",
    "\n",
    "<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8cfa6aa7-81be-4ad0-84c0-3b14ffd12e85",
   "metadata": {},
   "source": [
    "<a id=\"section4\"></a>\n",
    "## <font color=\"#92002A\">4. Implementación del script de entrada</font>\n",
    "\n",
    "<br>\n",
    "\n",
    "El *script* de entrada se encarga de recibir los datos enviados y pasárselos al modelo para realizar la predicción. Seguidamente, devuelve la respuesta del modelo al cliente. El *script* es específico para dicho modelo y debe entender los datos que el modelo espera y devuelve.\n",
    "\n",
    "Estas son las dos tareas que debe realizar en el script de entrada:\n",
    "\n",
    "* Cargar el modelo (mediante una función llamada `init`).\n",
    "* Ejecuta el modelo sobre los datos de entrada (mediante una función llamada `run`).\n",
    "\n",
    "A continuación, se muestra el *script* de entrada que hemos desarrollado para nuestro servicio:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "fcd638dd-9c53-448e-9686-47d41b9d1a8a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "SCRIPT_FOLDER = 'scripts/endpoint'\n",
    "\n",
    "if not os.path.exists(SCRIPT_FOLDER):\n",
    "    os.makedirs(SCRIPT_FOLDER)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "5718b32b-c73f-46b4-b2ba-141dc6c8ccfa",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting scripts/endpoint/recommend.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile scripts/endpoint/recommend.py\n",
    "import joblib\n",
    "import json\n",
    "import lightfm\n",
    "import numpy as np\n",
    "import os\n",
    "import time\n",
    "\n",
    "from scipy import sparse as sp\n",
    "\n",
    "def init():\n",
    "    global model\n",
    "    global model_data\n",
    "    global num_users\n",
    "    global num_items\n",
    "    global num_threads\n",
    "    \n",
    "    # AZUREML_MODEL_DIR is an environment variable created during deployment.\n",
    "    # It is the path to the model folder (./azureml-models/$MODEL_NAME/$VERSION)\n",
    "    # For multiple models, it points to the folder containing all deployed models (./azureml-models)\n",
    "    model_path = os.path.join(os.getenv(\"AZUREML_MODEL_DIR\"), \"model/model.pkl\")\n",
    "    model_data_path = os.path.join(os.getenv(\"AZUREML_MODEL_DIR\"), \"model/model_data.pkl\")\n",
    "    \n",
    "    model = joblib.load(model_path)\n",
    "    model_data = joblib.load(model_data_path)\n",
    "    \n",
    "    num_users = len([x for x in model_data['playlist_features_names'] if \"name:\" in x])\n",
    "    num_items = len([x for x in model_data['track_features_names'] if \"track:\" in x])\n",
    "    \n",
    "    num_threads = 3\n",
    "    \n",
    "\n",
    "# Obtiene de un modelo los items similares al indicado\n",
    "# mediante la similaridad coseno\n",
    "def similar_items(item_id, model, num_items, N=1000):\n",
    "    \"\"\"\n",
    "    :param item_id: Item del que obtener similares.\n",
    "    :param model: Modelo a emplear.\n",
    "    :param N: (Opcional) Número de items similares, por defecto se devuelven todos.\n",
    "    :return: Lista de tuplas en formato (PID,SCORE).\n",
    "    \"\"\"\n",
    "    \n",
    "    if N > 1000:\n",
    "        N = 1000\n",
    "    N += 200\n",
    "    (item_biased, item_representations) = model.get_item_representations()\n",
    "    \n",
    "    # Cosine Similarity\n",
    "    scores = item_representations.dot(item_representations[item_id])\n",
    "    item_norms = np.linalg.norm(item_representations, axis=1)\n",
    "    item_norms[item_norms == 0] = 1e-10    \n",
    "    scores /= item_norms\n",
    "    best = np.argpartition(scores, -N)[-N:]    \n",
    "    result = sorted(zip(best, scores[best] / item_norms[item_id]),\n",
    "                  key=lambda x: -x[1])\n",
    "    N -= 200\n",
    "    \n",
    "    return [{'item' : int(t), 'score' : float(s)} for (t,s) in result if t < num_items and t != item_id][:N]\n",
    "\n",
    "\n",
    "# Obtiene de un modelo los usuarios similares al indicado\n",
    "# mediante la similaridad coseno\n",
    "def similar_users(user_id, model, num_users, N=1000):\n",
    "    \"\"\"\n",
    "    :param item_id: Usuario del que obtener similares.\n",
    "    :param model: Modelo a emplear.\n",
    "    :param num_items: Número total de usuarios\n",
    "    :param N: (Opcional) Número de usuarios similares, por defecto se devuelven todos.\n",
    "    :return: Lista de tuplas en formato (PID,SCORE).\n",
    "    \"\"\"\n",
    "    \n",
    "    if N > 1000:\n",
    "        N = 1000\n",
    "    N += 200\n",
    "    \n",
    "    (user_biased, user_representations) = model.get_user_representations()\n",
    "    \n",
    "    # Cosine Similarity\n",
    "    scores = user_representations.dot(user_representations[user_id])\n",
    "    user_norms = np.linalg.norm(user_representations, axis=1)\n",
    "    user_norms[user_norms == 0] = 1e-10    \n",
    "    scores /= user_norms\n",
    "    best = np.argpartition(scores, -N)[-N:] \n",
    "    \n",
    "    result = sorted(zip(best, scores[best] / user_norms[user_id]),\n",
    "                  key=lambda x: -x[1])\n",
    "    N -= 200\n",
    "    \n",
    "    return [{'user' : int(u), 'score' : float(s)} for (u,s) in result if u < num_users and u != user_id][:N]\n",
    "\n",
    "\n",
    "def get_similar_user_tags(tag_id, model, N=100):\n",
    "    # Define similarity as the cosine of the angle\n",
    "    # between the tag latent vectors\n",
    "\n",
    "    # Normalize the vectors to unit length\n",
    "    tag_embeddings = (model.user_embeddings.T\n",
    "                      / np.linalg.norm(model.user_embeddings, axis=1)).T\n",
    "\n",
    "    query_embedding = tag_embeddings[tag_id]\n",
    "    similarity = np.dot(tag_embeddings, query_embedding)\n",
    "    most_similar = np.argsort(-similarity)[1:N+1]\n",
    "    \n",
    "    result = []\n",
    "    for user_id in most_similar:\n",
    "        result.append({'user_tag_id' : int(user_id), 'score' : float(similarity[user_id])})\n",
    "\n",
    "    return result\n",
    "\n",
    "\n",
    "def get_similar_item_tags(tag_id, model, N=100):\n",
    "    # Define similarity as the cosine of the angle\n",
    "    # between the tag latent vectors\n",
    "\n",
    "    # Normalize the vectors to unit length\n",
    "    tag_embeddings = (model.item_embeddings.T\n",
    "                      / np.linalg.norm(model.item_embeddings, axis=1)).T\n",
    "\n",
    "    query_embedding = tag_embeddings[tag_id]\n",
    "    similarity = np.dot(tag_embeddings, query_embedding)\n",
    "    most_similar = np.argsort(-similarity)[1:N+1]\n",
    "    \n",
    "    result = []\n",
    "    for item_id in most_similar:\n",
    "        result.append({'item_tag_id' : int(item_id), 'score' : float(similarity[item_id])})\n",
    "\n",
    "    return result\n",
    "\n",
    "\n",
    "def format_userfeats(userfeat_ids, num_user_features):\n",
    "    normalised_val = 1.0 \n",
    "    new_user_features = np.zeros(num_user_features)\n",
    "    \n",
    "    for i in userfeat_ids:\n",
    "        new_user_features[i] = normalised_val\n",
    "    \n",
    "    new_user_features = sp.csr_matrix(new_user_features)\n",
    "      \n",
    "    return new_user_features\n",
    "\n",
    "\n",
    "def make_newuser_recomm(userfeat_ids, model, i_features, num_items, n_recs=1000, n_threads=1):\n",
    "    if n_recs > 1000:\n",
    "        n_recs = 1000\n",
    "        \n",
    "    total_userfeats = len(model.get_user_representations()[0])\n",
    "    new_user_features = format_userfeats(userfeat_ids, total_userfeats)\n",
    "    \n",
    "    scores = model.predict(0, np.arange(num_items), user_features=new_user_features, \n",
    "                           item_features=model_data['track_features'], num_threads=n_threads)\n",
    "    items = list(np.argsort(scores)[::-1])[:n_recs]\n",
    "    \n",
    "    result = []    \n",
    "    for item_id in items:\n",
    "        result.append({'item_id' : int(item_id), 'score' : float(scores[item_id])})\n",
    "        \n",
    "    return result\n",
    "\n",
    "\n",
    "def make_user_recomm(user_id, model, u_features, i_features, num_items, n_recs=1000, n_threads=1):\n",
    "    if n_recs > 1000:\n",
    "        n_recs = 1000\n",
    "        \n",
    "    scores = model.predict(user_id, np.arange(num_items), user_features=u_features, \n",
    "                           item_features=i_features, num_threads=n_threads)\n",
    "    items = list(np.argsort(scores)[::-1])[:n_recs]\n",
    "    \n",
    "    result = []    \n",
    "    for item_id in items:\n",
    "        result.append({'item_id' : int(item_id), 'score' : float(scores[item_id])})\n",
    "        \n",
    "    return result\n",
    "\n",
    "\n",
    "\n",
    "def run(raw_data):\n",
    "    input_json = json.loads(raw_data)\n",
    "    \n",
    "    if all(k in input_json.keys() for k in (\"data\",\"action\")):    \n",
    "        data = input_json[\"data\"]\n",
    "        action = input_json[\"action\"]\n",
    "        start_time = time.time()\n",
    "        response = {}    \n",
    "        \n",
    "        try:\n",
    "            if action == 'similar_items':\n",
    "                data = json.loads(raw_data)[\"data\"]\n",
    "                result = similar_items(data,model,num_items)\n",
    "                response['result'] = result \n",
    "            elif action == 'similar_users':\n",
    "                data = json.loads(raw_data)[\"data\"]\n",
    "                result = similar_users(data,model,num_users)\n",
    "                response['result'] = result\n",
    "            elif action == 'similar_item_tags':\n",
    "                data = json.loads(raw_data)[\"data\"]\n",
    "                result = get_similar_item_tags(data,model)\n",
    "                response['result'] = result\n",
    "            elif action == 'similar_user_tags':\n",
    "                data = json.loads(raw_data)[\"data\"]\n",
    "                result = get_similar_user_tags(data,model)\n",
    "                response['result'] = result\n",
    "            elif action == 'make_prediction_newuser':\n",
    "                data = json.loads(raw_data)[\"data\"]\n",
    "                result = make_newuser_recomm(data, model, model_data['track_features'], \n",
    "                                             num_items, n_threads=num_threads)\n",
    "                response['result'] = result\n",
    "            elif action == 'make_prediction_user':\n",
    "                data = json.loads(raw_data)[\"data\"]\n",
    "                result = make_user_recomm(data, model, model_data['playlist_features'], \n",
    "                                          model_data['track_features'], num_items,\n",
    "                                          n_threads=num_threads)\n",
    "                response['result'] = result\n",
    "            else:\n",
    "                response['error'] = \"Wrong invocation.\"\n",
    "            \n",
    "        except Exception as e:\n",
    "            response['error'] = str(e)\n",
    "            \n",
    "        response['execution_time'] = time.time() - start_time\n",
    "\n",
    "        return response\n",
    "    else:\n",
    "        return {'error' : 'Wrong invocation.'}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29ba5941-2aa3-4e29-ba8c-afc6a781e427",
   "metadata": {},
   "source": [
    "<br>\n",
    "\n",
    "El script `recomend.py`, puede realizar las siguientes tareas:\n",
    "\n",
    "1) Encontrar pistas similares a una dada.\n",
    "2) Encontrar playlists similares a una dada.\n",
    "3) Buscar características de pistas similares a la indicada.\n",
    "4) Buscar características de playlists similares a la indicada.\n",
    "5) Realizar predicciones sobre una playlist conocida (está en nuestro conjunto de datos).\n",
    "6) Realizar predicciones a una nueva playlist (no se dispone información de ella).\n",
    "\n",
    "<br>\n",
    "\n",
    "Con el *script* de entrada ya definido, continuamos con la creación del punto de conexión."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d7367f55-983b-410b-9c67-6d92dea3cbae",
   "metadata": {},
   "source": [
    "<div style=\"text-align: right\">\n",
    "<a href=\"#indice\"><font size=5><i class=\"fa fa-arrow-circle-up\" aria-hidden=\"true\" style=\"color:#92002A\"></i></font></a>\n",
    "</div>\n",
    "\n",
    "---\n",
    "\n",
    "<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "61ff4e92-889e-47e4-997a-2f80b589377c",
   "metadata": {},
   "source": [
    "<a id=\"section5\"></a>\n",
    "## <font color=\"#92002A\">5. Creación del punto de conexión</font>\n",
    "\n",
    "<br>\n",
    "\n",
    "Para crear el punto de conexión, primero vamos a definir un *AciWebservice*. Este elemento representa un modelo de aprendizaje automático implementado como punto de conexión de servicio web en *Azure Container Instances*."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "53672f35-cc66-4901-b251-642ade7e768d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from azureml.core.webservice import AciWebservice\n",
    "\n",
    "# create deployment config i.e. compute resources\n",
    "aciconfig = AciWebservice.deploy_configuration(\n",
    "    cpu_cores=2,\n",
    "    memory_gb=12,\n",
    "    tags={\"data\": \"MPD\", \"method\": \"lightfm\"},\n",
    "    description=\"Recommend playlists with LightFM\",\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac0947c1-00d6-4b4c-8478-8661b1e5c228",
   "metadata": {},
   "source": [
    "<br>\n",
    "\n",
    "Seguidamente, vamos a definir un *environment* de *Python* con los paquetes necesarios para la ejecución del *script* y lo guardaremos en un fichero *YML*:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "606641bb-594a-48f7-a1bc-a39fceb497ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "from azureml.core.conda_dependencies import CondaDependencies\n",
    "from azureml.core.environment import Environment\n",
    "\n",
    "serenditipyenv = CondaDependencies()\n",
    "serenditipyenv.set_python_version(\"3.8.10\")\n",
    "serenditipyenv.add_pip_package(\"lightfm==1.16.0\")\n",
    "serenditipyenv.add_pip_package(\"numpy\")\n",
    "serenditipyenv.add_pip_package(\"scipy\")\n",
    "\n",
    "ENV_FILE_PATH = os.path.join(SCRIPT_FOLDER, 'serenditipyenv.yml')\n",
    "with open(ENV_FILE_PATH,\"w\") as f: \n",
    "    f.write(serenditipyenv.serialize_to_string())\n",
    "\n",
    "# Create environment\n",
    "env = Environment.from_conda_specification(\n",
    "    name=\"serenditipyenv\", \n",
    "    file_path=ENV_FILE_PATH)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15cb295b-ee50-4408-abae-0e702be09f70",
   "metadata": {},
   "source": [
    "<br>\n",
    "\n",
    "Por último, definimos la configuración de inferencia y desplegamos el servicio (punto de conexión):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "620b02a5-bbc7-435e-968f-b12049be6ae0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tips: You can try get_logs(): https://aka.ms/debugimage#dockerlog or local deployment: https://aka.ms/debugimage#debug-locally to debug if deployment takes longer than 10 minutes.\n",
      "Running\n",
      "2021-09-01 15:39:58+02:00 Creating Container Registry if not exists.\n",
      "2021-09-01 15:39:58+02:00 Registering the environment.\n",
      "2021-09-01 15:40:00+02:00 Use the existing image.\n",
      "2021-09-01 15:40:00+02:00 Generating deployment configuration.\n",
      "2021-09-01 15:40:01+02:00 Submitting deployment to compute.\n",
      "2021-09-01 15:40:06+02:00 Checking the status of deployment serendipity-recsys-model-fab6..\n",
      "2021-09-01 15:46:40+02:00 Checking the status of inference endpoint serendipity-recsys-model-fab6.\n",
      "Succeeded\n",
      "ACI service creation operation finished, operation \"Succeeded\"\n",
      "Wall time: 7min 29s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "import uuid\n",
    "from azureml.core.model import InferenceConfig\n",
    "from azureml.core.model import Model\n",
    "\n",
    "model = Model(workspace=ws, name=model_name, version=model_version)\n",
    "\n",
    "# create an inference config i.e. the scoring script and environment\n",
    "inference_config = InferenceConfig(\n",
    "    entry_script=\"recommend.py\", \n",
    "    source_directory=SCRIPT_FOLDER,\n",
    "    environment=env)\n",
    "\n",
    "# deploy the service\n",
    "service_name = \"serendipity-recsys-model-\" + str(uuid.uuid4())[:4]\n",
    "service = Model.deploy(\n",
    "    workspace=ws,\n",
    "    name=service_name,\n",
    "    models=[model],\n",
    "    inference_config=inference_config,\n",
    "    deployment_config=aciconfig,\n",
    ")\n",
    "\n",
    "service.wait_for_deployment(show_output=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9864334b-f1f4-4465-b0d0-0c40da63b329",
   "metadata": {},
   "source": [
    "<br>\n",
    "\n",
    "Una vez que está disponible nuestro punto de conexión al modelo que hemos creado, vamos a probar que funciona correctamente. Para ello, vamos a realizar una predicción a la playlist `1000002`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "9a846dfc-a7ae-4c5f-a96f-16ca0a75c5cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import requests\n",
    "\n",
    "input_data = {\n",
    "    \"action\" : \"make_prediction_user\",\n",
    "    \"data\" : 1000002\n",
    "}\n",
    "\n",
    "input_data = json.dumps(input_data)\n",
    "headers = {\"Content-Type\": \"application/json\"}\n",
    "\n",
    "resp = requests.post(service.scoring_uri, input_data, headers=headers)\n",
    "resp_json = json.loads(resp.text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "2e035984-2441-43a4-9f84-001ea565e5a2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2.2925477027893066"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "resp_json['execution_time']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "b37171e3-e5fa-4ee0-8b6f-477882e3de1e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'item_id': 5411, 'score': -110.66651916503906},\n",
       " {'item_id': 14272, 'score': -110.7023696899414},\n",
       " {'item_id': 5313, 'score': -110.80838012695312},\n",
       " {'item_id': 1330, 'score': -110.93283081054688},\n",
       " {'item_id': 6164, 'score': -111.12303161621094},\n",
       " {'item_id': 6044, 'score': -111.17489624023438},\n",
       " {'item_id': 20870, 'score': -111.26298522949219},\n",
       " {'item_id': 3764, 'score': -111.34845733642578},\n",
       " {'item_id': 5397, 'score': -111.37860870361328},\n",
       " {'item_id': 3806, 'score': -111.46448516845703}]"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "resp_json['result'][:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3acd4cba-36a3-4d10-878d-5f3c195226c6",
   "metadata": {},
   "source": [
    "<br>\n",
    "\n",
    "En caso de querer eliminar el punto de conexión que acabamos de crear, para evitar gastos innecesarios, basta con ejecutar la siguiente celda:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "8df84881-088c-41ff-8aa3-f86472c818b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# if you want to keep workspace and only delete endpoint (it will incur cost while running)\n",
    "service.delete()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "abcd14fb-458d-4fb4-82ee-6f2bd2353fd9",
   "metadata": {},
   "source": [
    "<div style=\"text-align: right\">\n",
    "<a href=\"#indice\"><font size=5><i class=\"fa fa-arrow-circle-up\" aria-hidden=\"true\" style=\"color:#92002A\"></i></font></a>\n",
    "</div>\n",
    "\n",
    "---\n",
    "\n",
    "<div style=\"text-align: right\"> <font size=6><i class=\"fa fa-graduation-cap\" aria-hidden=\"true\" style=\"color:#92002A\"></i> </font></div>"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
